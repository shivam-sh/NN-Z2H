{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "tokens = sorted(list(set(''.join(words))))\n",
    "ttoi = { token:index+1 for index,token in enumerate(tokens) }; ttoi['.'] = 0\n",
    "itot = { index:token for token,index in ttoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "import torch\n",
    "\n",
    "bi_in, bi_out = [], []\n",
    "tri_in, tri_out = [], []\n",
    "\n",
    "for word in words:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    for in1, out in zip(word, word[1:]):\n",
    "        in1 = ttoi[in1]\n",
    "        out = ttoi[out]\n",
    "\n",
    "        bi_in.append(in1)\n",
    "        bi_out.append(out)\n",
    "\n",
    "    word = ['.'] + list(word)\n",
    "    for (in1, in2), out in zip(zip(word, word[1:]), word[2:]):\n",
    "        in1 = ttoi[in1]\n",
    "        in2 = ttoi[in2]\n",
    "        out = ttoi[out]\n",
    "\n",
    "        tri_in.append(27 * in1 + in2)\n",
    "        tri_out.append(out)\n",
    "\n",
    "bi_in = torch.tensor(bi_in)\n",
    "bi_out = torch.tensor(bi_out)\n",
    "tri_in = torch.tensor(tri_in).to(torch.device('mps'))\n",
    "tri_out = torch.tensor(tri_out).to(torch.device('mps'))\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "bi_train_ids, bi_dev_ids, bi_test_ids = torch.utils.data.random_split(range(bi_out.shape[0]), [0.8, 0.1, 0.1], generator=g)\n",
    "tri_train_ids, tri_dev_ids, tri_test_ids = torch.utils.data.random_split(range(tri_out.shape[0]), [0.8, 0.1, 0.1], generator=g)\n",
    "\n",
    "bi_train_in, bi_train_out = bi_in[bi_train_ids], bi_out[bi_train_ids]\n",
    "bi_test_in, bi_test_out = bi_in[bi_test_ids], bi_out[bi_test_ids]\n",
    "bi_dev_in, bi_dev_out = bi_in[bi_dev_ids], bi_out[bi_dev_ids]\n",
    "\n",
    "tri_train_in, tri_train_out = tri_in[tri_train_ids], tri_out[tri_train_ids]\n",
    "tri_test_in, tri_test_out = tri_in[tri_test_ids], tri_out[tri_test_ids]\n",
    "tri_dev_in, tri_dev_out = tri_in[tri_dev_ids], tri_out[tri_dev_ids]\n",
    "\n",
    "# def bi_sample(inp, out, idx):\n",
    "#     ch1 = itot[inp[idx].item()]\n",
    "#     ch2 = itot[out[idx].item()]\n",
    "#     print(f'{ch1}:{ch2}')\n",
    "\n",
    "# def tri_sample(inp, out, idx):\n",
    "#     ch1 = itot[int(inp[idx].item() / len(itot.items()))] + itot[inp[idx].item() % len(itot.items())]\n",
    "#     ch2 = itot[out[idx].item()]\n",
    "#     print(f'{ch1}:{ch2}')\n",
    "\n",
    "# for i in range(len(tri_train_out)):\n",
    "#     bi_sample(bi_train_in, bi_train_out, i)\n",
    "# for i in range(len(tri_train_out)):\n",
    "#     tri_sample(tri_train_in, tri_train_out, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def softmax(logits):\n",
    "    counts = logits.exp()\n",
    "    return counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "def nll(probs, labels: torch.Tensor, weights: torch.Tensor | None = None, reg: float = 0):\n",
    "    if reg and not weights is None:\n",
    "        return -probs[torch.arange(labels.nelement()), labels].log().mean() + reg * (weights**2).mean()\n",
    "    else:\n",
    "        return -probs[torch.arange(labels.nelement()), labels].log().mean()\n",
    "    \n",
    "cross_entropy = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up bigram\n",
    "g = torch.Generator().manual_seed(0)\n",
    "bi_weights = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] - [Train]: 3.767875, [Dev]: 3.806053, [Test]: 3.813841\n",
      "Loss[1] - [Train]: 3.357414, [Dev]: 3.522970, [Test]: 3.532342\n",
      "Loss[2] - [Train]: 3.148772, [Dev]: 3.417052, [Test]: 3.425639\n",
      "Loss[3] - [Train]: 3.019019, [Dev]: 3.359639, [Test]: 3.366506\n",
      "Loss[4] - [Train]: 2.929806, [Dev]: 3.320242, [Test]: 3.325183\n",
      "Loss[5] - [Train]: 2.862049, [Dev]: 3.292201, [Test]: 3.295558\n",
      "Loss[6] - [Train]: 2.808945, [Dev]: 3.272936, [Test]: 3.274992\n",
      "Loss[7] - [Train]: 2.766779, [Dev]: 3.260004, [Test]: 3.260945\n",
      "Loss[8] - [Train]: 2.732884, [Dev]: 3.251523, [Test]: 3.251503\n",
      "Loss[9] - [Train]: 2.705285, [Dev]: 3.246052, [Test]: 3.245222\n",
      "Loss[10] - [Train]: 2.682480, [Dev]: 3.242569, [Test]: 3.241071\n",
      "Loss[11] - [Train]: 2.663318, [Dev]: 3.240383, [Test]: 3.238346\n",
      "Loss[12] - [Train]: 2.646944, [Dev]: 3.239054, [Test]: 3.236586\n",
      "Loss[13] - [Train]: 2.632752, [Dev]: 3.238323, [Test]: 3.235516\n",
      "Loss[14] - [Train]: 2.620317, [Dev]: 3.238041, [Test]: 3.234968\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(15):\n",
    "\n",
    "    # forward pass\n",
    "    logits = bi_weights[bi_train_in,:]\n",
    "    probs = softmax(logits)\n",
    "    loss = nll(probs, bi_train_out) #, bi_weights, reg=0.3)\n",
    "    print(f'Loss[{k}] - [Train]: {loss.item():4f}, [Dev]: {nll(probs, bi_dev_out):4f}, [Test]: {nll(probs, bi_test_out):4f}')\n",
    "\n",
    "    # backward pass\n",
    "    bi_weights.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    bi_weights.data += -50 * bi_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kedan.\n",
      "fdnnakayxaderyna.\n",
      "salzaer.\n",
      "rion.\n",
      "kefwwjunn.\n",
      "zuala.\n",
      "kanocojaylinilexa.\n",
      "m.\n",
      "cyl.\n",
      "kar.\n"
     ]
    }
   ],
   "source": [
    "def bigram(W):\n",
    "    g = torch.Generator().manual_seed(0)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        inp = 0\n",
    "        out = []\n",
    "\n",
    "        while True:\n",
    "            logits = W[[inp],:]\n",
    "            p = softmax(logits)\n",
    "\n",
    "            inp = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            out.append(itot[inp])\n",
    "            if inp == 0:\n",
    "                break\n",
    "        \n",
    "        print(''.join(out))\n",
    "\n",
    "bigram(bi_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up trigram network\n",
    "g = torch.Generator(device=torch.device('mps')).manual_seed(0)\n",
    "tri_weights = torch.randn((27**2, 27), generator=g, requires_grad=True, device=torch.device('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] - [Train]: 4.163890, [Dev]: 3.785672\n",
      "Loss[1] - [Train]: 3.950311, [Dev]: 3.665133\n",
      "Loss[2] - [Train]: 3.820945, [Dev]: 3.603215\n",
      "Loss[3] - [Train]: 3.718646, [Dev]: 3.568808\n",
      "Loss[4] - [Train]: 3.630326, [Dev]: 3.540179\n",
      "Loss[5] - [Train]: 3.554028, [Dev]: 3.520751\n",
      "Loss[6] - [Train]: 3.488144, [Dev]: 3.504799\n",
      "Loss[7] - [Train]: 3.431180, [Dev]: 3.493918\n",
      "Loss[8] - [Train]: 3.381530, [Dev]: 3.484564\n",
      "Loss[9] - [Train]: 3.337733, [Dev]: 3.477659\n",
      "Loss[10] - [Train]: 3.298646, [Dev]: 3.471278\n",
      "Loss[11] - [Train]: 3.263435, [Dev]: 3.466229\n",
      "Loss[12] - [Train]: 3.231478, [Dev]: 3.461488\n",
      "Loss[13] - [Train]: 3.202294, [Dev]: 3.457610\n",
      "Loss[14] - [Train]: 3.175498, [Dev]: 3.453983\n",
      "Loss[15] - [Train]: 3.150773, [Dev]: 3.450948\n",
      "Loss[16] - [Train]: 3.127857, [Dev]: 3.448121\n",
      "Loss[17] - [Train]: 3.106528, [Dev]: 3.445708\n",
      "Loss[18] - [Train]: 3.086597, [Dev]: 3.443464\n",
      "Loss[19] - [Train]: 3.067906, [Dev]: 3.441510\n",
      "Loss[20] - [Train]: 3.050319, [Dev]: 3.439692\n",
      "Loss[21] - [Train]: 3.033721, [Dev]: 3.438084\n",
      "Loss[22] - [Train]: 3.018013, [Dev]: 3.436589\n",
      "Loss[23] - [Train]: 3.003112, [Dev]: 3.435249\n",
      "Loss[24] - [Train]: 2.988945, [Dev]: 3.434005\n",
      "Loss[25] - [Train]: 2.975449, [Dev]: 3.432883\n",
      "Loss[26] - [Train]: 2.962572, [Dev]: 3.431844\n",
      "Loss[27] - [Train]: 2.950264, [Dev]: 3.430903\n",
      "Loss[28] - [Train]: 2.938485, [Dev]: 3.430037\n",
      "Loss[29] - [Train]: 2.927196, [Dev]: 3.429254\n",
      "Loss[30] - [Train]: 2.916365, [Dev]: 3.428535\n",
      "Loss[31] - [Train]: 2.905963, [Dev]: 3.427887\n",
      "Loss[32] - [Train]: 2.895961, [Dev]: 3.427296\n",
      "Loss[33] - [Train]: 2.886336, [Dev]: 3.426765\n",
      "Loss[34] - [Train]: 2.877066, [Dev]: 3.426287\n",
      "Loss[35] - [Train]: 2.868130, [Dev]: 3.425860\n",
      "Loss[36] - [Train]: 2.859509, [Dev]: 3.425479\n",
      "Loss[37] - [Train]: 2.851186, [Dev]: 3.425141\n",
      "Loss[38] - [Train]: 2.843145, [Dev]: 3.424844\n",
      "Loss[39] - [Train]: 2.835371, [Dev]: 3.424585\n",
      "Loss[40] - [Train]: 2.827850, [Dev]: 3.424360\n",
      "Loss[41] - [Train]: 2.820570, [Dev]: 3.424167\n",
      "Loss[42] - [Train]: 2.813518, [Dev]: 3.424004\n",
      "Loss[43] - [Train]: 2.806683, [Dev]: 3.423868\n",
      "Loss[44] - [Train]: 2.800054, [Dev]: 3.423757\n",
      "Loss[45] - [Train]: 2.793622, [Dev]: 3.423669\n",
      "Loss[46] - [Train]: 2.787378, [Dev]: 3.423603\n",
      "Loss[47] - [Train]: 2.781313, [Dev]: 3.423555\n",
      "Loss[48] - [Train]: 2.775418, [Dev]: 3.423525\n",
      "Loss[49] - [Train]: 2.769686, [Dev]: 3.423511\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    # forward Pass\n",
    "    logits = tri_weights[tri_train_in,:]\n",
    "    p = softmax(logits)\n",
    "    loss = nll(p, tri_train_out, weights=tri_weights, reg=0.4)\n",
    "    print(f'Loss[{i}] - [Train]: {loss.item():4f}, [Dev]: {nll(p, tri_dev_out):4f}')\n",
    "\n",
    "    # backward Pass\n",
    "    tri_weights.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    tri_weights.data += -100 * tri_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Loss: 2.508300542831421\n"
     ]
    }
   ],
   "source": [
    "logits = tri_weights[tri_test_in,:]\n",
    "p = softmax(logits)\n",
    "loss = nll(p, tri_test_out, weights=tri_weights)\n",
    "print(f'Test Set Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alan.\n",
      "laila.\n",
      "eztuptfbjspjfmbgocazikfzntoi.\n",
      "brawpzhtojsbelle.\n",
      "oltkyhy.\n",
      "re.\n",
      "ely.\n",
      "ara.\n",
      "ohc.\n",
      "chkffkvdptvmxeaocnkon.\n"
     ]
    }
   ],
   "source": [
    "def trigram(W):\n",
    "    g = torch.Generator(device=torch.device('mps')).manual_seed(0)\n",
    "\n",
    "    for _ in range(10):\n",
    "        in1, in2 = 0, 0\n",
    "        out = []\n",
    "\n",
    "        while True:\n",
    "            logints = W[[27 * in1 + in2],:]\n",
    "            p = softmax(logints)\n",
    "            o = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            in1, in2 = in2, o\n",
    "\n",
    "            out.append(itot[o])\n",
    "\n",
    "            if o == 0:\n",
    "                break\n",
    "        \n",
    "        print(''.join(out))\n",
    "\n",
    "trigram(tri_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up trigram network\n",
    "g = torch.Generator(device=torch.device('mps')).manual_seed(0)\n",
    "tri_weights = torch.randn((27**2, 27), generator=g, requires_grad=True, device=torch.device('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] - [Train]: 3.768633\n",
      "Loss[1] - [Train]: 3.564515\n",
      "Loss[2] - [Train]: 3.442633\n",
      "Loss[3] - [Train]: 3.346857\n",
      "Loss[4] - [Train]: 3.264207\n",
      "Loss[5] - [Train]: 3.192926\n",
      "Loss[6] - [Train]: 3.131569\n",
      "Loss[7] - [Train]: 3.078767\n",
      "Loss[8] - [Train]: 3.033004\n",
      "Loss[9] - [Train]: 2.992866\n",
      "Loss[10] - [Train]: 2.957241\n",
      "Loss[11] - [Train]: 2.925319\n",
      "Loss[12] - [Train]: 2.896499\n",
      "Loss[13] - [Train]: 2.870317\n",
      "Loss[14] - [Train]: 2.846403\n",
      "Loss[15] - [Train]: 2.824452\n",
      "Loss[16] - [Train]: 2.804211\n",
      "Loss[17] - [Train]: 2.785466\n",
      "Loss[18] - [Train]: 2.768036\n",
      "Loss[19] - [Train]: 2.751765\n",
      "Loss[20] - [Train]: 2.736524\n",
      "Loss[21] - [Train]: 2.722201\n",
      "Loss[22] - [Train]: 2.708700\n",
      "Loss[23] - [Train]: 2.695941\n",
      "Loss[24] - [Train]: 2.683854\n",
      "Loss[25] - [Train]: 2.672382\n",
      "Loss[26] - [Train]: 2.661471\n",
      "Loss[27] - [Train]: 2.651077\n",
      "Loss[28] - [Train]: 2.641161\n",
      "Loss[29] - [Train]: 2.631688\n",
      "Loss[30] - [Train]: 2.622627\n",
      "Loss[31] - [Train]: 2.613952\n",
      "Loss[32] - [Train]: 2.605637\n",
      "Loss[33] - [Train]: 2.597660\n",
      "Loss[34] - [Train]: 2.590001\n",
      "Loss[35] - [Train]: 2.582640\n",
      "Loss[36] - [Train]: 2.575562\n",
      "Loss[37] - [Train]: 2.568750\n",
      "Loss[38] - [Train]: 2.562191\n",
      "Loss[39] - [Train]: 2.555869\n",
      "Loss[40] - [Train]: 2.549774\n",
      "Loss[41] - [Train]: 2.543893\n",
      "Loss[42] - [Train]: 2.538215\n",
      "Loss[43] - [Train]: 2.532730\n",
      "Loss[44] - [Train]: 2.527429\n",
      "Loss[45] - [Train]: 2.522303\n",
      "Loss[46] - [Train]: 2.517343\n",
      "Loss[47] - [Train]: 2.512541\n",
      "Loss[48] - [Train]: 2.507891\n",
      "Loss[49] - [Train]: 2.503384\n",
      "Loss[50] - [Train]: 2.499015\n",
      "Loss[51] - [Train]: 2.494778\n",
      "Loss[52] - [Train]: 2.490666\n",
      "Loss[53] - [Train]: 2.486674\n",
      "Loss[54] - [Train]: 2.482796\n",
      "Loss[55] - [Train]: 2.479028\n",
      "Loss[56] - [Train]: 2.475366\n",
      "Loss[57] - [Train]: 2.471804\n",
      "Loss[58] - [Train]: 2.468339\n",
      "Loss[59] - [Train]: 2.464967\n",
      "Loss[60] - [Train]: 2.461683\n",
      "Loss[61] - [Train]: 2.458484\n",
      "Loss[62] - [Train]: 2.455368\n",
      "Loss[63] - [Train]: 2.452329\n",
      "Loss[64] - [Train]: 2.449366\n",
      "Loss[65] - [Train]: 2.446476\n",
      "Loss[66] - [Train]: 2.443656\n",
      "Loss[67] - [Train]: 2.440902\n",
      "Loss[68] - [Train]: 2.438214\n",
      "Loss[69] - [Train]: 2.435587\n",
      "Loss[70] - [Train]: 2.433020\n",
      "Loss[71] - [Train]: 2.430511\n",
      "Loss[72] - [Train]: 2.428058\n",
      "Loss[73] - [Train]: 2.425658\n",
      "Loss[74] - [Train]: 2.423310\n",
      "Loss[75] - [Train]: 2.421012\n",
      "Loss[76] - [Train]: 2.418762\n",
      "Loss[77] - [Train]: 2.416559\n",
      "Loss[78] - [Train]: 2.414401\n",
      "Loss[79] - [Train]: 2.412286\n",
      "Loss[80] - [Train]: 2.410214\n",
      "Loss[81] - [Train]: 2.408182\n",
      "Loss[82] - [Train]: 2.406190\n",
      "Loss[83] - [Train]: 2.404236\n",
      "Loss[84] - [Train]: 2.402319\n",
      "Loss[85] - [Train]: 2.400438\n",
      "Loss[86] - [Train]: 2.398592\n",
      "Loss[87] - [Train]: 2.396779\n",
      "Loss[88] - [Train]: 2.394999\n",
      "Loss[89] - [Train]: 2.393251\n",
      "Loss[90] - [Train]: 2.391534\n",
      "Loss[91] - [Train]: 2.389847\n",
      "Loss[92] - [Train]: 2.388189\n",
      "Loss[93] - [Train]: 2.386559\n",
      "Loss[94] - [Train]: 2.384957\n",
      "Loss[95] - [Train]: 2.383381\n",
      "Loss[96] - [Train]: 2.381832\n",
      "Loss[97] - [Train]: 2.380308\n",
      "Loss[98] - [Train]: 2.378808\n",
      "Loss[99] - [Train]: 2.377333\n",
      "Loss[100] - [Train]: 2.375881\n",
      "Loss[101] - [Train]: 2.374452\n",
      "Loss[102] - [Train]: 2.373045\n",
      "Loss[103] - [Train]: 2.371660\n",
      "Loss[104] - [Train]: 2.370296\n",
      "Loss[105] - [Train]: 2.368952\n",
      "Loss[106] - [Train]: 2.367629\n",
      "Loss[107] - [Train]: 2.366325\n",
      "Loss[108] - [Train]: 2.365041\n",
      "Loss[109] - [Train]: 2.363775\n",
      "Loss[110] - [Train]: 2.362527\n",
      "Loss[111] - [Train]: 2.361298\n",
      "Loss[112] - [Train]: 2.360085\n",
      "Loss[113] - [Train]: 2.358890\n",
      "Loss[114] - [Train]: 2.357712\n",
      "Loss[115] - [Train]: 2.356549\n",
      "Loss[116] - [Train]: 2.355403\n",
      "Loss[117] - [Train]: 2.354272\n",
      "Loss[118] - [Train]: 2.353157\n",
      "Loss[119] - [Train]: 2.352057\n",
      "Loss[120] - [Train]: 2.350971\n",
      "Loss[121] - [Train]: 2.349900\n",
      "Loss[122] - [Train]: 2.348842\n",
      "Loss[123] - [Train]: 2.347798\n",
      "Loss[124] - [Train]: 2.346768\n",
      "Loss[125] - [Train]: 2.345751\n",
      "Loss[126] - [Train]: 2.344747\n",
      "Loss[127] - [Train]: 2.343755\n",
      "Loss[128] - [Train]: 2.342776\n",
      "Loss[129] - [Train]: 2.341810\n",
      "Loss[130] - [Train]: 2.340855\n",
      "Loss[131] - [Train]: 2.339911\n",
      "Loss[132] - [Train]: 2.338979\n",
      "Loss[133] - [Train]: 2.338059\n",
      "Loss[134] - [Train]: 2.337150\n",
      "Loss[135] - [Train]: 2.336251\n",
      "Loss[136] - [Train]: 2.335363\n",
      "Loss[137] - [Train]: 2.334486\n",
      "Loss[138] - [Train]: 2.333619\n",
      "Loss[139] - [Train]: 2.332762\n",
      "Loss[140] - [Train]: 2.331914\n",
      "Loss[141] - [Train]: 2.331077\n",
      "Loss[142] - [Train]: 2.330249\n",
      "Loss[143] - [Train]: 2.329431\n",
      "Loss[144] - [Train]: 2.328621\n",
      "Loss[145] - [Train]: 2.327821\n",
      "Loss[146] - [Train]: 2.327030\n",
      "Loss[147] - [Train]: 2.326247\n",
      "Loss[148] - [Train]: 2.325473\n",
      "Loss[149] - [Train]: 2.324708\n",
      "Loss[150] - [Train]: 2.323950\n",
      "Loss[151] - [Train]: 2.323201\n",
      "Loss[152] - [Train]: 2.322460\n",
      "Loss[153] - [Train]: 2.321727\n",
      "Loss[154] - [Train]: 2.321002\n",
      "Loss[155] - [Train]: 2.320284\n",
      "Loss[156] - [Train]: 2.319574\n",
      "Loss[157] - [Train]: 2.318871\n",
      "Loss[158] - [Train]: 2.318176\n",
      "Loss[159] - [Train]: 2.317487\n",
      "Loss[160] - [Train]: 2.316806\n",
      "Loss[161] - [Train]: 2.316132\n",
      "Loss[162] - [Train]: 2.315465\n",
      "Loss[163] - [Train]: 2.314804\n",
      "Loss[164] - [Train]: 2.314150\n",
      "Loss[165] - [Train]: 2.313502\n",
      "Loss[166] - [Train]: 2.312861\n",
      "Loss[167] - [Train]: 2.312227\n",
      "Loss[168] - [Train]: 2.311598\n",
      "Loss[169] - [Train]: 2.310976\n",
      "Loss[170] - [Train]: 2.310359\n",
      "Loss[171] - [Train]: 2.309749\n",
      "Loss[172] - [Train]: 2.309144\n",
      "Loss[173] - [Train]: 2.308546\n",
      "Loss[174] - [Train]: 2.307953\n",
      "Loss[175] - [Train]: 2.307366\n",
      "Loss[176] - [Train]: 2.306784\n",
      "Loss[177] - [Train]: 2.306207\n",
      "Loss[178] - [Train]: 2.305636\n",
      "Loss[179] - [Train]: 2.305071\n",
      "Loss[180] - [Train]: 2.304510\n",
      "Loss[181] - [Train]: 2.303955\n",
      "Loss[182] - [Train]: 2.303405\n",
      "Loss[183] - [Train]: 2.302860\n",
      "Loss[184] - [Train]: 2.302320\n",
      "Loss[185] - [Train]: 2.301784\n",
      "Loss[186] - [Train]: 2.301254\n",
      "Loss[187] - [Train]: 2.300728\n",
      "Loss[188] - [Train]: 2.300207\n",
      "Loss[189] - [Train]: 2.299691\n",
      "Loss[190] - [Train]: 2.299179\n",
      "Loss[191] - [Train]: 2.298671\n",
      "Loss[192] - [Train]: 2.298169\n",
      "Loss[193] - [Train]: 2.297670\n",
      "Loss[194] - [Train]: 2.297176\n",
      "Loss[195] - [Train]: 2.296686\n",
      "Loss[196] - [Train]: 2.296201\n",
      "Loss[197] - [Train]: 2.295719\n",
      "Loss[198] - [Train]: 2.295242\n",
      "Loss[199] - [Train]: 2.294768\n",
      "Loss[200] - [Train]: 2.294299\n",
      "Loss[201] - [Train]: 2.293834\n",
      "Loss[202] - [Train]: 2.293372\n",
      "Loss[203] - [Train]: 2.292915\n",
      "Loss[204] - [Train]: 2.292461\n",
      "Loss[205] - [Train]: 2.292011\n",
      "Loss[206] - [Train]: 2.291565\n",
      "Loss[207] - [Train]: 2.291122\n",
      "Loss[208] - [Train]: 2.290684\n",
      "Loss[209] - [Train]: 2.290248\n",
      "Loss[210] - [Train]: 2.289816\n",
      "Loss[211] - [Train]: 2.289388\n",
      "Loss[212] - [Train]: 2.288963\n",
      "Loss[213] - [Train]: 2.288542\n",
      "Loss[214] - [Train]: 2.288124\n",
      "Loss[215] - [Train]: 2.287709\n",
      "Loss[216] - [Train]: 2.287297\n",
      "Loss[217] - [Train]: 2.286889\n",
      "Loss[218] - [Train]: 2.286484\n",
      "Loss[219] - [Train]: 2.286083\n",
      "Loss[220] - [Train]: 2.285684\n",
      "Loss[221] - [Train]: 2.285289\n",
      "Loss[222] - [Train]: 2.284896\n",
      "Loss[223] - [Train]: 2.284507\n",
      "Loss[224] - [Train]: 2.284121\n",
      "Loss[225] - [Train]: 2.283737\n",
      "Loss[226] - [Train]: 2.283357\n",
      "Loss[227] - [Train]: 2.282979\n",
      "Loss[228] - [Train]: 2.282604\n",
      "Loss[229] - [Train]: 2.282233\n",
      "Loss[230] - [Train]: 2.281864\n",
      "Loss[231] - [Train]: 2.281497\n",
      "Loss[232] - [Train]: 2.281134\n",
      "Loss[233] - [Train]: 2.280773\n",
      "Loss[234] - [Train]: 2.280415\n",
      "Loss[235] - [Train]: 2.280060\n",
      "Loss[236] - [Train]: 2.279707\n",
      "Loss[237] - [Train]: 2.279357\n",
      "Loss[238] - [Train]: 2.279010\n",
      "Loss[239] - [Train]: 2.278664\n",
      "Loss[240] - [Train]: 2.278322\n",
      "Loss[241] - [Train]: 2.277982\n",
      "Loss[242] - [Train]: 2.277645\n",
      "Loss[243] - [Train]: 2.277309\n",
      "Loss[244] - [Train]: 2.276977\n",
      "Loss[245] - [Train]: 2.276646\n",
      "Loss[246] - [Train]: 2.276319\n",
      "Loss[247] - [Train]: 2.275993\n",
      "Loss[248] - [Train]: 2.275670\n",
      "Loss[249] - [Train]: 2.275349\n",
      "Loss[250] - [Train]: 2.275030\n",
      "Loss[251] - [Train]: 2.274714\n",
      "Loss[252] - [Train]: 2.274400\n",
      "Loss[253] - [Train]: 2.274087\n",
      "Loss[254] - [Train]: 2.273777\n",
      "Loss[255] - [Train]: 2.273470\n",
      "Loss[256] - [Train]: 2.273164\n",
      "Loss[257] - [Train]: 2.272861\n",
      "Loss[258] - [Train]: 2.272560\n",
      "Loss[259] - [Train]: 2.272260\n",
      "Loss[260] - [Train]: 2.271963\n",
      "Loss[261] - [Train]: 2.271668\n",
      "Loss[262] - [Train]: 2.271375\n",
      "Loss[263] - [Train]: 2.271084\n",
      "Loss[264] - [Train]: 2.270794\n",
      "Loss[265] - [Train]: 2.270507\n",
      "Loss[266] - [Train]: 2.270222\n",
      "Loss[267] - [Train]: 2.269938\n",
      "Loss[268] - [Train]: 2.269657\n",
      "Loss[269] - [Train]: 2.269377\n",
      "Loss[270] - [Train]: 2.269099\n",
      "Loss[271] - [Train]: 2.268823\n",
      "Loss[272] - [Train]: 2.268549\n",
      "Loss[273] - [Train]: 2.268277\n",
      "Loss[274] - [Train]: 2.268006\n",
      "Loss[275] - [Train]: 2.267737\n",
      "Loss[276] - [Train]: 2.267470\n",
      "Loss[277] - [Train]: 2.267205\n",
      "Loss[278] - [Train]: 2.266942\n",
      "Loss[279] - [Train]: 2.266680\n",
      "Loss[280] - [Train]: 2.266420\n",
      "Loss[281] - [Train]: 2.266161\n",
      "Loss[282] - [Train]: 2.265904\n",
      "Loss[283] - [Train]: 2.265649\n",
      "Loss[284] - [Train]: 2.265395\n",
      "Loss[285] - [Train]: 2.265143\n",
      "Loss[286] - [Train]: 2.264893\n",
      "Loss[287] - [Train]: 2.264644\n",
      "Loss[288] - [Train]: 2.264397\n",
      "Loss[289] - [Train]: 2.264151\n",
      "Loss[290] - [Train]: 2.263907\n",
      "Loss[291] - [Train]: 2.263664\n",
      "Loss[292] - [Train]: 2.263423\n",
      "Loss[293] - [Train]: 2.263183\n",
      "Loss[294] - [Train]: 2.262945\n",
      "Loss[295] - [Train]: 2.262708\n",
      "Loss[296] - [Train]: 2.262473\n",
      "Loss[297] - [Train]: 2.262239\n",
      "Loss[298] - [Train]: 2.262007\n",
      "Loss[299] - [Train]: 2.261776\n",
      "Loss[300] - [Train]: 2.261546\n",
      "Loss[301] - [Train]: 2.261318\n",
      "Loss[302] - [Train]: 2.261091\n",
      "Loss[303] - [Train]: 2.260866\n",
      "Loss[304] - [Train]: 2.260642\n",
      "Loss[305] - [Train]: 2.260419\n",
      "Loss[306] - [Train]: 2.260197\n",
      "Loss[307] - [Train]: 2.259977\n",
      "Loss[308] - [Train]: 2.259758\n",
      "Loss[309] - [Train]: 2.259541\n",
      "Loss[310] - [Train]: 2.259324\n",
      "Loss[311] - [Train]: 2.259109\n",
      "Loss[312] - [Train]: 2.258895\n",
      "Loss[313] - [Train]: 2.258683\n",
      "Loss[314] - [Train]: 2.258472\n",
      "Loss[315] - [Train]: 2.258262\n",
      "Loss[316] - [Train]: 2.258053\n",
      "Loss[317] - [Train]: 2.257845\n",
      "Loss[318] - [Train]: 2.257639\n",
      "Loss[319] - [Train]: 2.257434\n",
      "Loss[320] - [Train]: 2.257230\n",
      "Loss[321] - [Train]: 2.257026\n",
      "Loss[322] - [Train]: 2.256825\n",
      "Loss[323] - [Train]: 2.256624\n",
      "Loss[324] - [Train]: 2.256425\n",
      "Loss[325] - [Train]: 2.256226\n",
      "Loss[326] - [Train]: 2.256029\n",
      "Loss[327] - [Train]: 2.255833\n",
      "Loss[328] - [Train]: 2.255638\n",
      "Loss[329] - [Train]: 2.255444\n",
      "Loss[330] - [Train]: 2.255251\n",
      "Loss[331] - [Train]: 2.255059\n",
      "Loss[332] - [Train]: 2.254868\n",
      "Loss[333] - [Train]: 2.254679\n",
      "Loss[334] - [Train]: 2.254490\n",
      "Loss[335] - [Train]: 2.254302\n",
      "Loss[336] - [Train]: 2.254116\n",
      "Loss[337] - [Train]: 2.253930\n",
      "Loss[338] - [Train]: 2.253745\n",
      "Loss[339] - [Train]: 2.253562\n",
      "Loss[340] - [Train]: 2.253379\n",
      "Loss[341] - [Train]: 2.253198\n",
      "Loss[342] - [Train]: 2.253017\n",
      "Loss[343] - [Train]: 2.252837\n",
      "Loss[344] - [Train]: 2.252658\n",
      "Loss[345] - [Train]: 2.252481\n",
      "Loss[346] - [Train]: 2.252304\n",
      "Loss[347] - [Train]: 2.252128\n",
      "Loss[348] - [Train]: 2.251953\n",
      "Loss[349] - [Train]: 2.251779\n",
      "Loss[350] - [Train]: 2.251606\n",
      "Loss[351] - [Train]: 2.251434\n",
      "Loss[352] - [Train]: 2.251262\n",
      "Loss[353] - [Train]: 2.251092\n",
      "Loss[354] - [Train]: 2.250922\n",
      "Loss[355] - [Train]: 2.250753\n",
      "Loss[356] - [Train]: 2.250586\n",
      "Loss[357] - [Train]: 2.250419\n",
      "Loss[358] - [Train]: 2.250253\n",
      "Loss[359] - [Train]: 2.250087\n",
      "Loss[360] - [Train]: 2.249923\n",
      "Loss[361] - [Train]: 2.249759\n",
      "Loss[362] - [Train]: 2.249597\n",
      "Loss[363] - [Train]: 2.249435\n",
      "Loss[364] - [Train]: 2.249274\n",
      "Loss[365] - [Train]: 2.249113\n",
      "Loss[366] - [Train]: 2.248954\n",
      "Loss[367] - [Train]: 2.248795\n",
      "Loss[368] - [Train]: 2.248637\n",
      "Loss[369] - [Train]: 2.248480\n",
      "Loss[370] - [Train]: 2.248324\n",
      "Loss[371] - [Train]: 2.248168\n",
      "Loss[372] - [Train]: 2.248013\n",
      "Loss[373] - [Train]: 2.247860\n",
      "Loss[374] - [Train]: 2.247706\n",
      "Loss[375] - [Train]: 2.247554\n",
      "Loss[376] - [Train]: 2.247402\n",
      "Loss[377] - [Train]: 2.247251\n",
      "Loss[378] - [Train]: 2.247101\n",
      "Loss[379] - [Train]: 2.246951\n",
      "Loss[380] - [Train]: 2.246802\n",
      "Loss[381] - [Train]: 2.246654\n",
      "Loss[382] - [Train]: 2.246507\n",
      "Loss[383] - [Train]: 2.246360\n",
      "Loss[384] - [Train]: 2.246214\n",
      "Loss[385] - [Train]: 2.246069\n",
      "Loss[386] - [Train]: 2.245924\n",
      "Loss[387] - [Train]: 2.245780\n",
      "Loss[388] - [Train]: 2.245637\n",
      "Loss[389] - [Train]: 2.245495\n",
      "Loss[390] - [Train]: 2.245353\n",
      "Loss[391] - [Train]: 2.245211\n",
      "Loss[392] - [Train]: 2.245071\n",
      "Loss[393] - [Train]: 2.244931\n",
      "Loss[394] - [Train]: 2.244792\n",
      "Loss[395] - [Train]: 2.244653\n",
      "Loss[396] - [Train]: 2.244515\n",
      "Loss[397] - [Train]: 2.244378\n",
      "Loss[398] - [Train]: 2.244241\n",
      "Loss[399] - [Train]: 2.244105\n",
      "Loss[400] - [Train]: 2.243970\n",
      "Loss[401] - [Train]: 2.243835\n",
      "Loss[402] - [Train]: 2.243701\n",
      "Loss[403] - [Train]: 2.243567\n",
      "Loss[404] - [Train]: 2.243434\n",
      "Loss[405] - [Train]: 2.243302\n",
      "Loss[406] - [Train]: 2.243170\n",
      "Loss[407] - [Train]: 2.243039\n",
      "Loss[408] - [Train]: 2.242908\n",
      "Loss[409] - [Train]: 2.242779\n",
      "Loss[410] - [Train]: 2.242649\n",
      "Loss[411] - [Train]: 2.242520\n",
      "Loss[412] - [Train]: 2.242392\n",
      "Loss[413] - [Train]: 2.242265\n",
      "Loss[414] - [Train]: 2.242137\n",
      "Loss[415] - [Train]: 2.242011\n",
      "Loss[416] - [Train]: 2.241885\n",
      "Loss[417] - [Train]: 2.241759\n",
      "Loss[418] - [Train]: 2.241635\n",
      "Loss[419] - [Train]: 2.241510\n",
      "Loss[420] - [Train]: 2.241386\n",
      "Loss[421] - [Train]: 2.241263\n",
      "Loss[422] - [Train]: 2.241141\n",
      "Loss[423] - [Train]: 2.241018\n",
      "Loss[424] - [Train]: 2.240897\n",
      "Loss[425] - [Train]: 2.240776\n",
      "Loss[426] - [Train]: 2.240655\n",
      "Loss[427] - [Train]: 2.240535\n",
      "Loss[428] - [Train]: 2.240415\n",
      "Loss[429] - [Train]: 2.240296\n",
      "Loss[430] - [Train]: 2.240178\n",
      "Loss[431] - [Train]: 2.240060\n",
      "Loss[432] - [Train]: 2.239942\n",
      "Loss[433] - [Train]: 2.239825\n",
      "Loss[434] - [Train]: 2.239709\n",
      "Loss[435] - [Train]: 2.239593\n",
      "Loss[436] - [Train]: 2.239477\n",
      "Loss[437] - [Train]: 2.239362\n",
      "Loss[438] - [Train]: 2.239248\n",
      "Loss[439] - [Train]: 2.239134\n",
      "Loss[440] - [Train]: 2.239020\n",
      "Loss[441] - [Train]: 2.238907\n",
      "Loss[442] - [Train]: 2.238794\n",
      "Loss[443] - [Train]: 2.238682\n",
      "Loss[444] - [Train]: 2.238570\n",
      "Loss[445] - [Train]: 2.238459\n",
      "Loss[446] - [Train]: 2.238348\n",
      "Loss[447] - [Train]: 2.238238\n",
      "Loss[448] - [Train]: 2.238128\n",
      "Loss[449] - [Train]: 2.238018\n",
      "Loss[450] - [Train]: 2.237909\n",
      "Loss[451] - [Train]: 2.237801\n",
      "Loss[452] - [Train]: 2.237693\n",
      "Loss[453] - [Train]: 2.237585\n",
      "Loss[454] - [Train]: 2.237478\n",
      "Loss[455] - [Train]: 2.237371\n",
      "Loss[456] - [Train]: 2.237265\n",
      "Loss[457] - [Train]: 2.237159\n",
      "Loss[458] - [Train]: 2.237053\n",
      "Loss[459] - [Train]: 2.236948\n",
      "Loss[460] - [Train]: 2.236843\n",
      "Loss[461] - [Train]: 2.236739\n",
      "Loss[462] - [Train]: 2.236635\n",
      "Loss[463] - [Train]: 2.236532\n",
      "Loss[464] - [Train]: 2.236429\n",
      "Loss[465] - [Train]: 2.236326\n",
      "Loss[466] - [Train]: 2.236224\n",
      "Loss[467] - [Train]: 2.236122\n",
      "Loss[468] - [Train]: 2.236021\n",
      "Loss[469] - [Train]: 2.235920\n",
      "Loss[470] - [Train]: 2.235819\n",
      "Loss[471] - [Train]: 2.235719\n",
      "Loss[472] - [Train]: 2.235619\n",
      "Loss[473] - [Train]: 2.235519\n",
      "Loss[474] - [Train]: 2.235420\n",
      "Loss[475] - [Train]: 2.235322\n",
      "Loss[476] - [Train]: 2.235224\n",
      "Loss[477] - [Train]: 2.235126\n",
      "Loss[478] - [Train]: 2.235028\n",
      "Loss[479] - [Train]: 2.234931\n",
      "Loss[480] - [Train]: 2.234834\n",
      "Loss[481] - [Train]: 2.234738\n",
      "Loss[482] - [Train]: 2.234642\n",
      "Loss[483] - [Train]: 2.234546\n",
      "Loss[484] - [Train]: 2.234451\n",
      "Loss[485] - [Train]: 2.234356\n",
      "Loss[486] - [Train]: 2.234262\n",
      "Loss[487] - [Train]: 2.234167\n",
      "Loss[488] - [Train]: 2.234073\n",
      "Loss[489] - [Train]: 2.233979\n",
      "Loss[490] - [Train]: 2.233886\n",
      "Loss[491] - [Train]: 2.233794\n",
      "Loss[492] - [Train]: 2.233701\n",
      "Loss[493] - [Train]: 2.233609\n",
      "Loss[494] - [Train]: 2.233517\n",
      "Loss[495] - [Train]: 2.233426\n",
      "Loss[496] - [Train]: 2.233335\n",
      "Loss[497] - [Train]: 2.233244\n",
      "Loss[498] - [Train]: 2.233154\n",
      "Loss[499] - [Train]: 2.233063\n",
      "Loss[500] - [Train]: 2.232974\n",
      "Loss[501] - [Train]: 2.232884\n",
      "Loss[502] - [Train]: 2.232795\n",
      "Loss[503] - [Train]: 2.232706\n",
      "Loss[504] - [Train]: 2.232618\n",
      "Loss[505] - [Train]: 2.232530\n",
      "Loss[506] - [Train]: 2.232442\n",
      "Loss[507] - [Train]: 2.232355\n",
      "Loss[508] - [Train]: 2.232267\n",
      "Loss[509] - [Train]: 2.232181\n",
      "Loss[510] - [Train]: 2.232094\n",
      "Loss[511] - [Train]: 2.232008\n",
      "Loss[512] - [Train]: 2.231922\n",
      "Loss[513] - [Train]: 2.231836\n",
      "Loss[514] - [Train]: 2.231751\n",
      "Loss[515] - [Train]: 2.231666\n",
      "Loss[516] - [Train]: 2.231581\n",
      "Loss[517] - [Train]: 2.231497\n",
      "Loss[518] - [Train]: 2.231413\n",
      "Loss[519] - [Train]: 2.231329\n",
      "Loss[520] - [Train]: 2.231246\n",
      "Loss[521] - [Train]: 2.231162\n",
      "Loss[522] - [Train]: 2.231080\n",
      "Loss[523] - [Train]: 2.230997\n",
      "Loss[524] - [Train]: 2.230914\n",
      "Loss[525] - [Train]: 2.230833\n",
      "Loss[526] - [Train]: 2.230751\n",
      "Loss[527] - [Train]: 2.230669\n",
      "Loss[528] - [Train]: 2.230588\n",
      "Loss[529] - [Train]: 2.230507\n",
      "Loss[530] - [Train]: 2.230427\n",
      "Loss[531] - [Train]: 2.230346\n",
      "Loss[532] - [Train]: 2.230266\n",
      "Loss[533] - [Train]: 2.230187\n",
      "Loss[534] - [Train]: 2.230107\n",
      "Loss[535] - [Train]: 2.230028\n",
      "Loss[536] - [Train]: 2.229949\n",
      "Loss[537] - [Train]: 2.229870\n",
      "Loss[538] - [Train]: 2.229792\n",
      "Loss[539] - [Train]: 2.229713\n",
      "Loss[540] - [Train]: 2.229636\n",
      "Loss[541] - [Train]: 2.229558\n",
      "Loss[542] - [Train]: 2.229481\n",
      "Loss[543] - [Train]: 2.229404\n",
      "Loss[544] - [Train]: 2.229327\n",
      "Loss[545] - [Train]: 2.229250\n",
      "Loss[546] - [Train]: 2.229174\n",
      "Loss[547] - [Train]: 2.229098\n",
      "Loss[548] - [Train]: 2.229022\n",
      "Loss[549] - [Train]: 2.228947\n",
      "Loss[550] - [Train]: 2.228871\n",
      "Loss[551] - [Train]: 2.228796\n",
      "Loss[552] - [Train]: 2.228722\n",
      "Loss[553] - [Train]: 2.228647\n",
      "Loss[554] - [Train]: 2.228573\n",
      "Loss[555] - [Train]: 2.228499\n",
      "Loss[556] - [Train]: 2.228425\n",
      "Loss[557] - [Train]: 2.228352\n",
      "Loss[558] - [Train]: 2.228278\n",
      "Loss[559] - [Train]: 2.228205\n",
      "Loss[560] - [Train]: 2.228132\n",
      "Loss[561] - [Train]: 2.228060\n",
      "Loss[562] - [Train]: 2.227987\n",
      "Loss[563] - [Train]: 2.227916\n",
      "Loss[564] - [Train]: 2.227844\n",
      "Loss[565] - [Train]: 2.227772\n",
      "Loss[566] - [Train]: 2.227700\n",
      "Loss[567] - [Train]: 2.227629\n",
      "Loss[568] - [Train]: 2.227558\n",
      "Loss[569] - [Train]: 2.227488\n",
      "Loss[570] - [Train]: 2.227417\n",
      "Loss[571] - [Train]: 2.227347\n",
      "Loss[572] - [Train]: 2.227277\n",
      "Loss[573] - [Train]: 2.227207\n",
      "Loss[574] - [Train]: 2.227137\n",
      "Loss[575] - [Train]: 2.227068\n",
      "Loss[576] - [Train]: 2.226999\n",
      "Loss[577] - [Train]: 2.226930\n",
      "Loss[578] - [Train]: 2.226861\n",
      "Loss[579] - [Train]: 2.226793\n",
      "Loss[580] - [Train]: 2.226725\n",
      "Loss[581] - [Train]: 2.226657\n",
      "Loss[582] - [Train]: 2.226589\n",
      "Loss[583] - [Train]: 2.226521\n",
      "Loss[584] - [Train]: 2.226454\n",
      "Loss[585] - [Train]: 2.226387\n",
      "Loss[586] - [Train]: 2.226320\n",
      "Loss[587] - [Train]: 2.226253\n",
      "Loss[588] - [Train]: 2.226187\n",
      "Loss[589] - [Train]: 2.226120\n",
      "Loss[590] - [Train]: 2.226054\n",
      "Loss[591] - [Train]: 2.225988\n",
      "Loss[592] - [Train]: 2.225922\n",
      "Loss[593] - [Train]: 2.225857\n",
      "Loss[594] - [Train]: 2.225792\n",
      "Loss[595] - [Train]: 2.225727\n",
      "Loss[596] - [Train]: 2.225662\n",
      "Loss[597] - [Train]: 2.225597\n",
      "Loss[598] - [Train]: 2.225533\n",
      "Loss[599] - [Train]: 2.225468\n",
      "Loss[600] - [Train]: 2.225404\n",
      "Loss[601] - [Train]: 2.225340\n",
      "Loss[602] - [Train]: 2.225276\n",
      "Loss[603] - [Train]: 2.225213\n",
      "Loss[604] - [Train]: 2.225150\n",
      "Loss[605] - [Train]: 2.225086\n",
      "Loss[606] - [Train]: 2.225024\n",
      "Loss[607] - [Train]: 2.224961\n",
      "Loss[608] - [Train]: 2.224898\n",
      "Loss[609] - [Train]: 2.224836\n",
      "Loss[610] - [Train]: 2.224774\n",
      "Loss[611] - [Train]: 2.224712\n",
      "Loss[612] - [Train]: 2.224650\n",
      "Loss[613] - [Train]: 2.224588\n",
      "Loss[614] - [Train]: 2.224527\n",
      "Loss[615] - [Train]: 2.224465\n",
      "Loss[616] - [Train]: 2.224404\n",
      "Loss[617] - [Train]: 2.224344\n",
      "Loss[618] - [Train]: 2.224283\n",
      "Loss[619] - [Train]: 2.224222\n",
      "Loss[620] - [Train]: 2.224162\n",
      "Loss[621] - [Train]: 2.224102\n",
      "Loss[622] - [Train]: 2.224042\n",
      "Loss[623] - [Train]: 2.223982\n",
      "Loss[624] - [Train]: 2.223922\n",
      "Loss[625] - [Train]: 2.223863\n",
      "Loss[626] - [Train]: 2.223804\n",
      "Loss[627] - [Train]: 2.223745\n",
      "Loss[628] - [Train]: 2.223686\n",
      "Loss[629] - [Train]: 2.223627\n",
      "Loss[630] - [Train]: 2.223568\n",
      "Loss[631] - [Train]: 2.223510\n",
      "Loss[632] - [Train]: 2.223452\n",
      "Loss[633] - [Train]: 2.223393\n",
      "Loss[634] - [Train]: 2.223336\n",
      "Loss[635] - [Train]: 2.223278\n",
      "Loss[636] - [Train]: 2.223220\n",
      "Loss[637] - [Train]: 2.223163\n",
      "Loss[638] - [Train]: 2.223106\n",
      "Loss[639] - [Train]: 2.223049\n",
      "Loss[640] - [Train]: 2.222991\n",
      "Loss[641] - [Train]: 2.222935\n",
      "Loss[642] - [Train]: 2.222878\n",
      "Loss[643] - [Train]: 2.222822\n",
      "Loss[644] - [Train]: 2.222765\n",
      "Loss[645] - [Train]: 2.222709\n",
      "Loss[646] - [Train]: 2.222654\n",
      "Loss[647] - [Train]: 2.222598\n",
      "Loss[648] - [Train]: 2.222542\n",
      "Loss[649] - [Train]: 2.222487\n",
      "Loss[650] - [Train]: 2.222431\n",
      "Loss[651] - [Train]: 2.222376\n",
      "Loss[652] - [Train]: 2.222322\n",
      "Loss[653] - [Train]: 2.222266\n",
      "Loss[654] - [Train]: 2.222212\n",
      "Loss[655] - [Train]: 2.222157\n",
      "Loss[656] - [Train]: 2.222103\n",
      "Loss[657] - [Train]: 2.222049\n",
      "Loss[658] - [Train]: 2.221995\n",
      "Loss[659] - [Train]: 2.221941\n",
      "Loss[660] - [Train]: 2.221887\n",
      "Loss[661] - [Train]: 2.221833\n",
      "Loss[662] - [Train]: 2.221780\n",
      "Loss[663] - [Train]: 2.221726\n",
      "Loss[664] - [Train]: 2.221673\n",
      "Loss[665] - [Train]: 2.221620\n",
      "Loss[666] - [Train]: 2.221567\n",
      "Loss[667] - [Train]: 2.221515\n",
      "Loss[668] - [Train]: 2.221462\n",
      "Loss[669] - [Train]: 2.221410\n",
      "Loss[670] - [Train]: 2.221357\n",
      "Loss[671] - [Train]: 2.221305\n",
      "Loss[672] - [Train]: 2.221253\n",
      "Loss[673] - [Train]: 2.221201\n",
      "Loss[674] - [Train]: 2.221149\n",
      "Loss[675] - [Train]: 2.221098\n",
      "Loss[676] - [Train]: 2.221046\n",
      "Loss[677] - [Train]: 2.220995\n",
      "Loss[678] - [Train]: 2.220944\n",
      "Loss[679] - [Train]: 2.220893\n",
      "Loss[680] - [Train]: 2.220842\n",
      "Loss[681] - [Train]: 2.220791\n",
      "Loss[682] - [Train]: 2.220741\n",
      "Loss[683] - [Train]: 2.220690\n",
      "Loss[684] - [Train]: 2.220640\n",
      "Loss[685] - [Train]: 2.220590\n",
      "Loss[686] - [Train]: 2.220540\n",
      "Loss[687] - [Train]: 2.220490\n",
      "Loss[688] - [Train]: 2.220440\n",
      "Loss[689] - [Train]: 2.220390\n",
      "Loss[690] - [Train]: 2.220340\n",
      "Loss[691] - [Train]: 2.220291\n",
      "Loss[692] - [Train]: 2.220242\n",
      "Loss[693] - [Train]: 2.220193\n",
      "Loss[694] - [Train]: 2.220144\n",
      "Loss[695] - [Train]: 2.220095\n",
      "Loss[696] - [Train]: 2.220046\n",
      "Loss[697] - [Train]: 2.219997\n",
      "Loss[698] - [Train]: 2.219949\n",
      "Loss[699] - [Train]: 2.219900\n",
      "Loss[700] - [Train]: 2.219852\n",
      "Loss[701] - [Train]: 2.219804\n",
      "Loss[702] - [Train]: 2.219756\n",
      "Loss[703] - [Train]: 2.219708\n",
      "Loss[704] - [Train]: 2.219660\n",
      "Loss[705] - [Train]: 2.219613\n",
      "Loss[706] - [Train]: 2.219565\n",
      "Loss[707] - [Train]: 2.219518\n",
      "Loss[708] - [Train]: 2.219471\n",
      "Loss[709] - [Train]: 2.219423\n",
      "Loss[710] - [Train]: 2.219377\n",
      "Loss[711] - [Train]: 2.219330\n",
      "Loss[712] - [Train]: 2.219283\n",
      "Loss[713] - [Train]: 2.219236\n",
      "Loss[714] - [Train]: 2.219190\n",
      "Loss[715] - [Train]: 2.219143\n",
      "Loss[716] - [Train]: 2.219097\n",
      "Loss[717] - [Train]: 2.219051\n",
      "Loss[718] - [Train]: 2.219005\n",
      "Loss[719] - [Train]: 2.218959\n",
      "Loss[720] - [Train]: 2.218913\n",
      "Loss[721] - [Train]: 2.218867\n",
      "Loss[722] - [Train]: 2.218822\n",
      "Loss[723] - [Train]: 2.218776\n",
      "Loss[724] - [Train]: 2.218731\n",
      "Loss[725] - [Train]: 2.218686\n",
      "Loss[726] - [Train]: 2.218641\n",
      "Loss[727] - [Train]: 2.218596\n",
      "Loss[728] - [Train]: 2.218551\n",
      "Loss[729] - [Train]: 2.218506\n",
      "Loss[730] - [Train]: 2.218461\n",
      "Loss[731] - [Train]: 2.218417\n",
      "Loss[732] - [Train]: 2.218372\n",
      "Loss[733] - [Train]: 2.218328\n",
      "Loss[734] - [Train]: 2.218284\n",
      "Loss[735] - [Train]: 2.218240\n",
      "Loss[736] - [Train]: 2.218196\n",
      "Loss[737] - [Train]: 2.218152\n",
      "Loss[738] - [Train]: 2.218108\n",
      "Loss[739] - [Train]: 2.218064\n",
      "Loss[740] - [Train]: 2.218021\n",
      "Loss[741] - [Train]: 2.217977\n",
      "Loss[742] - [Train]: 2.217934\n",
      "Loss[743] - [Train]: 2.217891\n",
      "Loss[744] - [Train]: 2.217848\n",
      "Loss[745] - [Train]: 2.217804\n",
      "Loss[746] - [Train]: 2.217762\n",
      "Loss[747] - [Train]: 2.217719\n",
      "Loss[748] - [Train]: 2.217676\n",
      "Loss[749] - [Train]: 2.217633\n",
      "Loss[750] - [Train]: 2.217591\n",
      "Loss[751] - [Train]: 2.217549\n",
      "Loss[752] - [Train]: 2.217506\n",
      "Loss[753] - [Train]: 2.217464\n",
      "Loss[754] - [Train]: 2.217422\n",
      "Loss[755] - [Train]: 2.217380\n",
      "Loss[756] - [Train]: 2.217338\n",
      "Loss[757] - [Train]: 2.217296\n",
      "Loss[758] - [Train]: 2.217255\n",
      "Loss[759] - [Train]: 2.217213\n",
      "Loss[760] - [Train]: 2.217172\n",
      "Loss[761] - [Train]: 2.217130\n",
      "Loss[762] - [Train]: 2.217089\n",
      "Loss[763] - [Train]: 2.217048\n",
      "Loss[764] - [Train]: 2.217007\n",
      "Loss[765] - [Train]: 2.216966\n",
      "Loss[766] - [Train]: 2.216925\n",
      "Loss[767] - [Train]: 2.216884\n",
      "Loss[768] - [Train]: 2.216843\n",
      "Loss[769] - [Train]: 2.216803\n",
      "Loss[770] - [Train]: 2.216762\n",
      "Loss[771] - [Train]: 2.216722\n",
      "Loss[772] - [Train]: 2.216681\n",
      "Loss[773] - [Train]: 2.216641\n",
      "Loss[774] - [Train]: 2.216601\n",
      "Loss[775] - [Train]: 2.216561\n",
      "Loss[776] - [Train]: 2.216521\n",
      "Loss[777] - [Train]: 2.216481\n",
      "Loss[778] - [Train]: 2.216442\n",
      "Loss[779] - [Train]: 2.216402\n",
      "Loss[780] - [Train]: 2.216362\n",
      "Loss[781] - [Train]: 2.216323\n",
      "Loss[782] - [Train]: 2.216284\n",
      "Loss[783] - [Train]: 2.216244\n",
      "Loss[784] - [Train]: 2.216205\n",
      "Loss[785] - [Train]: 2.216166\n",
      "Loss[786] - [Train]: 2.216127\n",
      "Loss[787] - [Train]: 2.216088\n",
      "Loss[788] - [Train]: 2.216049\n",
      "Loss[789] - [Train]: 2.216011\n",
      "Loss[790] - [Train]: 2.215972\n",
      "Loss[791] - [Train]: 2.215933\n",
      "Loss[792] - [Train]: 2.215895\n",
      "Loss[793] - [Train]: 2.215857\n",
      "Loss[794] - [Train]: 2.215818\n",
      "Loss[795] - [Train]: 2.215780\n",
      "Loss[796] - [Train]: 2.215742\n",
      "Loss[797] - [Train]: 2.215704\n",
      "Loss[798] - [Train]: 2.215666\n",
      "Loss[799] - [Train]: 2.215628\n",
      "Loss[800] - [Train]: 2.215590\n",
      "Loss[801] - [Train]: 2.215553\n",
      "Loss[802] - [Train]: 2.215515\n",
      "Loss[803] - [Train]: 2.215478\n",
      "Loss[804] - [Train]: 2.215440\n",
      "Loss[805] - [Train]: 2.215403\n",
      "Loss[806] - [Train]: 2.215366\n",
      "Loss[807] - [Train]: 2.215328\n",
      "Loss[808] - [Train]: 2.215292\n",
      "Loss[809] - [Train]: 2.215255\n",
      "Loss[810] - [Train]: 2.215218\n",
      "Loss[811] - [Train]: 2.215181\n",
      "Loss[812] - [Train]: 2.215144\n",
      "Loss[813] - [Train]: 2.215107\n",
      "Loss[814] - [Train]: 2.215071\n",
      "Loss[815] - [Train]: 2.215034\n",
      "Loss[816] - [Train]: 2.214998\n",
      "Loss[817] - [Train]: 2.214962\n",
      "Loss[818] - [Train]: 2.214925\n",
      "Loss[819] - [Train]: 2.214889\n",
      "Loss[820] - [Train]: 2.214853\n",
      "Loss[821] - [Train]: 2.214817\n",
      "Loss[822] - [Train]: 2.214781\n",
      "Loss[823] - [Train]: 2.214746\n",
      "Loss[824] - [Train]: 2.214710\n",
      "Loss[825] - [Train]: 2.214674\n",
      "Loss[826] - [Train]: 2.214638\n",
      "Loss[827] - [Train]: 2.214603\n",
      "Loss[828] - [Train]: 2.214567\n",
      "Loss[829] - [Train]: 2.214532\n",
      "Loss[830] - [Train]: 2.214497\n",
      "Loss[831] - [Train]: 2.214462\n",
      "Loss[832] - [Train]: 2.214426\n",
      "Loss[833] - [Train]: 2.214391\n",
      "Loss[834] - [Train]: 2.214356\n",
      "Loss[835] - [Train]: 2.214321\n",
      "Loss[836] - [Train]: 2.214287\n",
      "Loss[837] - [Train]: 2.214252\n",
      "Loss[838] - [Train]: 2.214217\n",
      "Loss[839] - [Train]: 2.214182\n",
      "Loss[840] - [Train]: 2.214148\n",
      "Loss[841] - [Train]: 2.214114\n",
      "Loss[842] - [Train]: 2.214079\n",
      "Loss[843] - [Train]: 2.214045\n",
      "Loss[844] - [Train]: 2.214010\n",
      "Loss[845] - [Train]: 2.213976\n",
      "Loss[846] - [Train]: 2.213942\n",
      "Loss[847] - [Train]: 2.213908\n",
      "Loss[848] - [Train]: 2.213874\n",
      "Loss[849] - [Train]: 2.213840\n",
      "Loss[850] - [Train]: 2.213807\n",
      "Loss[851] - [Train]: 2.213773\n",
      "Loss[852] - [Train]: 2.213739\n",
      "Loss[853] - [Train]: 2.213706\n",
      "Loss[854] - [Train]: 2.213672\n",
      "Loss[855] - [Train]: 2.213639\n",
      "Loss[856] - [Train]: 2.213606\n",
      "Loss[857] - [Train]: 2.213572\n",
      "Loss[858] - [Train]: 2.213539\n",
      "Loss[859] - [Train]: 2.213506\n",
      "Loss[860] - [Train]: 2.213473\n",
      "Loss[861] - [Train]: 2.213440\n",
      "Loss[862] - [Train]: 2.213407\n",
      "Loss[863] - [Train]: 2.213374\n",
      "Loss[864] - [Train]: 2.213341\n",
      "Loss[865] - [Train]: 2.213309\n",
      "Loss[866] - [Train]: 2.213276\n",
      "Loss[867] - [Train]: 2.213243\n",
      "Loss[868] - [Train]: 2.213211\n",
      "Loss[869] - [Train]: 2.213178\n",
      "Loss[870] - [Train]: 2.213146\n",
      "Loss[871] - [Train]: 2.213114\n",
      "Loss[872] - [Train]: 2.213082\n",
      "Loss[873] - [Train]: 2.213049\n",
      "Loss[874] - [Train]: 2.213017\n",
      "Loss[875] - [Train]: 2.212985\n",
      "Loss[876] - [Train]: 2.212953\n",
      "Loss[877] - [Train]: 2.212921\n",
      "Loss[878] - [Train]: 2.212889\n",
      "Loss[879] - [Train]: 2.212857\n",
      "Loss[880] - [Train]: 2.212826\n",
      "Loss[881] - [Train]: 2.212794\n",
      "Loss[882] - [Train]: 2.212763\n",
      "Loss[883] - [Train]: 2.212731\n",
      "Loss[884] - [Train]: 2.212700\n",
      "Loss[885] - [Train]: 2.212668\n",
      "Loss[886] - [Train]: 2.212637\n",
      "Loss[887] - [Train]: 2.212606\n",
      "Loss[888] - [Train]: 2.212575\n",
      "Loss[889] - [Train]: 2.212543\n",
      "Loss[890] - [Train]: 2.212512\n",
      "Loss[891] - [Train]: 2.212481\n",
      "Loss[892] - [Train]: 2.212451\n",
      "Loss[893] - [Train]: 2.212420\n",
      "Loss[894] - [Train]: 2.212389\n",
      "Loss[895] - [Train]: 2.212358\n",
      "Loss[896] - [Train]: 2.212327\n",
      "Loss[897] - [Train]: 2.212297\n",
      "Loss[898] - [Train]: 2.212266\n",
      "Loss[899] - [Train]: 2.212236\n",
      "Loss[900] - [Train]: 2.212205\n",
      "Loss[901] - [Train]: 2.212175\n",
      "Loss[902] - [Train]: 2.212145\n",
      "Loss[903] - [Train]: 2.212115\n",
      "Loss[904] - [Train]: 2.212084\n",
      "Loss[905] - [Train]: 2.212054\n",
      "Loss[906] - [Train]: 2.212024\n",
      "Loss[907] - [Train]: 2.211994\n",
      "Loss[908] - [Train]: 2.211964\n",
      "Loss[909] - [Train]: 2.211934\n",
      "Loss[910] - [Train]: 2.211904\n",
      "Loss[911] - [Train]: 2.211875\n",
      "Loss[912] - [Train]: 2.211845\n",
      "Loss[913] - [Train]: 2.211815\n",
      "Loss[914] - [Train]: 2.211786\n",
      "Loss[915] - [Train]: 2.211756\n",
      "Loss[916] - [Train]: 2.211727\n",
      "Loss[917] - [Train]: 2.211697\n",
      "Loss[918] - [Train]: 2.211668\n",
      "Loss[919] - [Train]: 2.211639\n",
      "Loss[920] - [Train]: 2.211609\n",
      "Loss[921] - [Train]: 2.211580\n",
      "Loss[922] - [Train]: 2.211551\n",
      "Loss[923] - [Train]: 2.211522\n",
      "Loss[924] - [Train]: 2.211493\n",
      "Loss[925] - [Train]: 2.211464\n",
      "Loss[926] - [Train]: 2.211435\n",
      "Loss[927] - [Train]: 2.211406\n",
      "Loss[928] - [Train]: 2.211378\n",
      "Loss[929] - [Train]: 2.211349\n",
      "Loss[930] - [Train]: 2.211320\n",
      "Loss[931] - [Train]: 2.211292\n",
      "Loss[932] - [Train]: 2.211263\n",
      "Loss[933] - [Train]: 2.211235\n",
      "Loss[934] - [Train]: 2.211206\n",
      "Loss[935] - [Train]: 2.211178\n",
      "Loss[936] - [Train]: 2.211149\n",
      "Loss[937] - [Train]: 2.211121\n",
      "Loss[938] - [Train]: 2.211093\n",
      "Loss[939] - [Train]: 2.211065\n",
      "Loss[940] - [Train]: 2.211037\n",
      "Loss[941] - [Train]: 2.211009\n",
      "Loss[942] - [Train]: 2.210981\n",
      "Loss[943] - [Train]: 2.210953\n",
      "Loss[944] - [Train]: 2.210925\n",
      "Loss[945] - [Train]: 2.210897\n",
      "Loss[946] - [Train]: 2.210869\n",
      "Loss[947] - [Train]: 2.210842\n",
      "Loss[948] - [Train]: 2.210814\n",
      "Loss[949] - [Train]: 2.210786\n",
      "Loss[950] - [Train]: 2.210759\n",
      "Loss[951] - [Train]: 2.210731\n",
      "Loss[952] - [Train]: 2.210704\n",
      "Loss[953] - [Train]: 2.210676\n",
      "Loss[954] - [Train]: 2.210649\n",
      "Loss[955] - [Train]: 2.210622\n",
      "Loss[956] - [Train]: 2.210594\n",
      "Loss[957] - [Train]: 2.210567\n",
      "Loss[958] - [Train]: 2.210540\n",
      "Loss[959] - [Train]: 2.210513\n",
      "Loss[960] - [Train]: 2.210486\n",
      "Loss[961] - [Train]: 2.210459\n",
      "Loss[962] - [Train]: 2.210432\n",
      "Loss[963] - [Train]: 2.210405\n",
      "Loss[964] - [Train]: 2.210378\n",
      "Loss[965] - [Train]: 2.210352\n",
      "Loss[966] - [Train]: 2.210325\n",
      "Loss[967] - [Train]: 2.210298\n",
      "Loss[968] - [Train]: 2.210272\n",
      "Loss[969] - [Train]: 2.210245\n",
      "Loss[970] - [Train]: 2.210218\n",
      "Loss[971] - [Train]: 2.210192\n",
      "Loss[972] - [Train]: 2.210165\n",
      "Loss[973] - [Train]: 2.210139\n",
      "Loss[974] - [Train]: 2.210113\n",
      "Loss[975] - [Train]: 2.210086\n",
      "Loss[976] - [Train]: 2.210060\n",
      "Loss[977] - [Train]: 2.210034\n",
      "Loss[978] - [Train]: 2.210008\n",
      "Loss[979] - [Train]: 2.209982\n",
      "Loss[980] - [Train]: 2.209956\n",
      "Loss[981] - [Train]: 2.209930\n",
      "Loss[982] - [Train]: 2.209904\n",
      "Loss[983] - [Train]: 2.209878\n",
      "Loss[984] - [Train]: 2.209852\n",
      "Loss[985] - [Train]: 2.209826\n",
      "Loss[986] - [Train]: 2.209800\n",
      "Loss[987] - [Train]: 2.209775\n",
      "Loss[988] - [Train]: 2.209749\n",
      "Loss[989] - [Train]: 2.209723\n",
      "Loss[990] - [Train]: 2.209698\n",
      "Loss[991] - [Train]: 2.209672\n",
      "Loss[992] - [Train]: 2.209647\n",
      "Loss[993] - [Train]: 2.209622\n",
      "Loss[994] - [Train]: 2.209596\n",
      "Loss[995] - [Train]: 2.209571\n",
      "Loss[996] - [Train]: 2.209546\n",
      "Loss[997] - [Train]: 2.209520\n",
      "Loss[998] - [Train]: 2.209495\n",
      "Loss[999] - [Train]: 2.209470\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # forward Pass\n",
    "    logits = tri_weights[tri_train_in,:]\n",
    "    loss = cross_entropy(logits, tri_train_out)\n",
    "    print(f'Loss[{i}] - [Train]: {loss.item():4f}')\n",
    "\n",
    "    # backward Pass\n",
    "    tri_weights.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    tri_weights.data += -100 * tri_weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Loss: 2.239563465118408\n"
     ]
    }
   ],
   "source": [
    "logits = tri_weights[tri_test_in,:]\n",
    "p = softmax(logits)\n",
    "loss = nll(p, tri_test_out, weights=tri_weights)\n",
    "print(f'Test Set Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alan.\n",
      "laila.\n",
      "ezamarie.\n",
      "saifmely.\n",
      "aziannator.\n",
      "brawsynto.\n",
      "seelle.\n",
      "oltayah.\n",
      "re.\n",
      "eighara.\n"
     ]
    }
   ],
   "source": [
    "def trigram(W):\n",
    "    g = torch.Generator(device=torch.device('mps')).manual_seed(0)\n",
    "\n",
    "    for _ in range(10):\n",
    "        in1, in2 = 0, 0\n",
    "        out = []\n",
    "\n",
    "        while True:\n",
    "            logints = W[[27 * in1 + in2],:]\n",
    "            p = softmax(logints)\n",
    "            o = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "            in1, in2 = in2, o\n",
    "\n",
    "            out.append(itot[o])\n",
    "\n",
    "            if o == 0:\n",
    "                break\n",
    "        \n",
    "        print(''.join(out))\n",
    "\n",
    "trigram(tri_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
